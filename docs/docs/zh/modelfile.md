# Ollama 模型文件

> [!NOTE]
> `Modelfile` 语法正在开发中

模型文件是创建和共享 Ollama 模型的蓝图。

## 目录

- [格式](#format)
- [示例](#examples)
- [指令](#instructions)
  - [FROM（必需）](#from-required)
    - [基于现有模型构建](#build-from-existing-model)
    - [从Safetensors模型构建](#build-from-a-safetensors-model)
    - [从GGUF文件构建](#build-from-a-gguf-file)
  - [参数](#parameter)
    - [有效的参数和值](#valid-parameters-and-values)
  - [模板](#template)
    - [模板变量](#template-variables)
  - [SYSTEM](#system)
  - [ADAPTER](#adapter)
  - [许可证](#license)
  - [MESSAGE](#message)
- [注意事项](#notes)

## 格式 {#format}

`Modelfile` 的格式：

```modelfile
# comment
INSTRUCTION arguments
```





| Instruction                         | Description                                                    |
| ----------------------------------- | -------------------------------------------------------------- |
| [`FROM`](#from-required) (required) | Defines the base model to use.                                 |
| [`PARAMETER`](#parameter)           | Sets the parameters for how Ollama will run the model.         |
| [`TEMPLATE`](#template)             | The full prompt template to be sent to the model.              |
| [`SYSTEM`](#system)                 | Specifies the system message that will be set in the template. |
| [`ADAPTER`](#adapter)               | Defines the (Q)LoRA adapters to apply to the model.            |
| [`LICENSE`](#license)               | Specifies the legal license.                                   |
| [`MESSAGE`](#message)               | Specify message history.                                       |


## 示例 {#examples}

### 基本 `Modelfile`

一个创建马里奥蓝图的 `Modelfile` 示例：

```modelfile
FROM llama3.2
# sets the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1
# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token
PARAMETER num_ctx 4096

# sets a custom system message to specify the behavior of the chat assistant
SYSTEM You are Mario from super mario bros, acting as an assistant.
```

要使用这个：

1. 将其保存为文件（例如 `Modelfile`）
2. `ollama create choose-a-model-name -f <location of the file e.g. ./Modelfile>'`
3. `ollama run choose-a-model-name`
4. 开始使用模型!

更多示例可在[示例目录](https://github.com/ollama/ollama/tree/main/examples)中找到。

要查看给定模型的Modelfile，请使用`ollama show --modelfile`命令。


  ```bash
  > ollama show --modelfile llama3.2
  # Modelfile generated by "ollama show"
  # To build a new Modelfile based on this one, replace the FROM line with:
  # FROM llama3.2:latest
  FROM /Users/pdevine/.ollama/models/blobs/sha256-00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29
  TEMPLATE """{{ if .System }}<|start_header_id|>system<|end_header_id|>

  {{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>

  {{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>

  {{ .Response }}<|eot_id|>"""
  PARAMETER stop "<|start_header_id|>"
  PARAMETER stop "<|end_header_id|>"
  PARAMETER stop "<|eot_id|>"
  PARAMETER stop "<|reserved_special_token"
  ```

## 指令 {#instructions}

### FROM（必需） {#from-required}

`FROM` 指令定义了在创建模型时要使用的基础模型。

```modelfile
FROM <model name>:<tag>
```

#### 基于现有模型构建 {#build-from-existing-model}

```modelfile
FROM llama3.2
```

可用的基础模型列表：

<https://github.com/ollama/ollama#model-library>
可以在以下位置找到其他模型：
<https://ollama.com/library>

#### 从Safetensors模型构建 {#build-from-a-safetensors-model}

```modelfile
FROM <model directory>
```

模型目录应包含支持架构的 Safetensors 权重。

目前支持的模型架构：
  * Llama（包括 Llama 2、Llama 3、Llama 3.1 和 Llama 3.2）
  * Mistral（包括 Mistral 1、Mistral 2 和 Mixtral）
  * Gemma（包括 Gemma 1 和 Gemma 2）
  * Phi3

#### 从GGUF文件构建 {#build-from-a-gguf-file}

```modelfile
FROM ./ollama-model.gguf
```

GGUF 文件的位置应指定为绝对路径或相对于 `Modelfile` 的位置。


### 参数 {#parameter}

`PARAMETER` 指令定义了一个在运行模型时可以设置的参数。

```modelfile
PARAMETER <parameter> <parametervalue>
```

#### 有效的参数和值 {#valid-parameters-and-values}


| Parameter      | Description                                                                                                                                                                                                                                             | Value Type | Example Usage        |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------- | -------------------- |
| mirostat       | Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)                                                                                                                                         | int        | mirostat 0           |
| mirostat_eta   | Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1)                        | float      | mirostat_eta 0.1     |
| mirostat_tau   | Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0)                                                                                                         | float      | mirostat_tau 5.0     |
| num_ctx        | Sets the size of the context window used to generate the next token. (Default: 2048)                                                                                                                                                                    | int        | num_ctx 4096         |
| repeat_last_n  | Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)                                                                                                                                           | int        | repeat_last_n 64     |
| repeat_penalty | Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)                                                                     | float      | repeat_penalty 1.1   |
| temperature    | The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)                                                                                                                                     | float      | temperature 0.7      |
| seed           | Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0)                                                                                       | int        | seed 42              |
| stop           | Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate `stop` parameters in a modelfile.                                      | string     | stop "AI assistant:" |
| tfs_z          | Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (default: 1)                                               | float      | tfs_z 1              |
| num_predict    | Maximum number of tokens to predict when generating text. (Default: 128, -1 = infinite generation, -2 = fill context)                                                                                                                                   | int        | num_predict 42       |
| top_k          | Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)                                                                        | int        | top_k 40             |
| top_p          | Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)                                                                 | float      | top_p 0.9            |
| min_p          | Alternative to the top_p, and aims to ensure a balance of quality and variety. The parameter *p* represents the minimum probability for a token to be considered, relative to the probability of the most likely token. For example, with *p*=0.05 and the most likely token having a probability of 0.9, logits with a value less than 0.045 are filtered out. (Default: 0.0) | float      | min_p 0.05            |


### 模板  {#template}

要传递给模型的完整提示模板 `TEMPLATE`。它可能包括（可选）系统消息、用户消息和模型的响应。注意：语法可能是模型特定的。模板使用 Go [模板语法](https://pkg.go.dev/text/template)。

#### 模板变量 {#template-variables}


| Variable          | Description                                                                                   |
| ----------------- | --------------------------------------------------------------------------------------------- |
| `{{ .System }}`   | The system message used to specify custom behavior.                                           |
| `{{ .Prompt }}`   | The user prompt message.                                                                      |
| `{{ .Response }}` | The response from the model. When generating a response, text after this variable is omitted. |





```
TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
"""
```

### SYSTEM 

`SYSTEM` 指令指定了模板中要使用的系统消息（如果适用）。

```modelfile
SYSTEM """<system message>"""
```

### ADAPTER

`ADAPTER` 指令指定了一个应应用于基础模型的微调 LoRA 适配器。适配器的值应为绝对路径或相对于 Modelfile 的路径。基础模型应使用 `FROM` 指令指定。如果基础模型与适配器微调时所用的基础模型不同，行为将不可预测。

#### Safetensor 适配器

```modelfile
ADAPTER <path to safetensor adapter>
```

目前支持的 Safetensor 适配器：
  * Llama（包括 Llama 2、Llama 3 和 Llama 3.1）
  * Mistral（包括 Mistral 1、Mistral 2 和 Mixtral）
  * Gemma（包括 Gemma 1 和 Gemma 2）

#### GGUF 适配器

```modelfile
ADAPTER ./ollama-lora.gguf
```

### 许可证 {#license}

`LICENSE` 指令允许你指定与此 Modelfile 一起使用的模型的共享或分发所依据的法律许可证。

```modelfile
LICENSE """
<license text>
"""
```

### MESSAGE

`MESSAGE` 指令允许你为模型指定消息历史，以便模型在响应时使用。通过多次使用 `MESSAGE` 命令来构建对话，可以引导模型以类似的方式回答。

```modelfile
MESSAGE <role> <message>
```

#### 有效的角色


| Role      | Description                                                  |
| --------- | ------------------------------------------------------------ |
| system    | Alternate way of providing the SYSTEM message for the model. |
| user      | An example message of what the user could have asked.        |
| assistant | An example message of how the model should respond.          |


#### 示例对话

```modelfile
MESSAGE user Is Toronto in Canada?
MESSAGE assistant yes
MESSAGE user Is Sacramento in Canada?
MESSAGE assistant no
MESSAGE user Is Ontario in Canada?
MESSAGE assistant yes
```

## 注意事项 {#notes}

- **`Modelfile` 不区分大小写**。在示例中，使用大写指令是为了更容易区分它与参数。
- 指令可以按任意顺序排列。在示例中，`FROM` 指令放在首位是为了保持可读性。

[1]: https://ollama.com/library